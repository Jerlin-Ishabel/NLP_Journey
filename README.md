## ğŸ“˜ Day 1 - Introduction to NLP & Tokenization

### âœ… Learning Summary

On **Day 1** of my NLP journey, I explored the **basics of Natural Language Processing (NLP)** and performed my first text-processing task â€” **Tokenization**.

Key takeaways:
- NLP is a field of AI focused on understanding human language.
- Applications: Chatbots, Translation, Spam detection, Sentiment Analysis.
- Tokenization is the process of splitting text into smaller units (words or sentences).

I learned how to use **NLTK** to tokenize words and sentences in Python.

---

### ğŸ“ Tasks Completed

| Task No. | Description |
|----------|-------------|
| âœ… Task 1 | Installed NLTK and downloaded necessary datasets (`punkt`) |
| âœ… Task 2 | Wrote Python script to tokenize text into words using `word_tokenize()` |
| âœ… Task 3 | Tokenized a paragraph into sentences using `sent_tokenize()` |
| âœ… Task 4 | Created and tested on custom input (my own sentence) |
| âœ… Task 5 | Observed how punctuation is handled in tokenization |

Script saved as: `day1_tokenization_practice.py`

---

### ğŸ’» Mini Project â€“ Tokenizer Web App

**Project Name**: `Mini NLP Tokenizer App`  
**Built With**: Python ğŸ + Streamlit ğŸš€ + NLTK ğŸ§ 

#### âœ… Features:
- Accepts paragraph input
- Displays:
  - Word Tokens
  - Sentence Tokens
  - Total word count
- Simple UI using Streamlit

#### ğŸ“‚ Files:
- `tokenizer_app.py`

#### â–¶ï¸ How to Run:
```bash
streamlit run tokenizer_app.py
