## 📘 Day 1 - Introduction to NLP & Tokenization

### ✅ Learning Summary

On **Day 1** of my NLP journey, I explored the **basics of Natural Language Processing (NLP)** and performed my first text-processing task — **Tokenization**.

Key takeaways:
- NLP is a field of AI focused on understanding human language.
- Applications: Chatbots, Translation, Spam detection, Sentiment Analysis.
- Tokenization is the process of splitting text into smaller units (words or sentences).

I learned how to use **NLTK** to tokenize words and sentences in Python.

---

### 📝 Tasks Completed

| Task No. | Description |
|----------|-------------|
| ✅ Task 1 | Installed NLTK and downloaded necessary datasets (`punkt`) |
| ✅ Task 2 | Wrote Python script to tokenize text into words using `word_tokenize()` |
| ✅ Task 3 | Tokenized a paragraph into sentences using `sent_tokenize()` |
| ✅ Task 4 | Created and tested on custom input (my own sentence) |
| ✅ Task 5 | Observed how punctuation is handled in tokenization |

Script saved as: `day1_tokenization_practice.py`

---

### 💻 Mini Project – Tokenizer Web App

**Project Name**: `Mini NLP Tokenizer App`  
**Built With**: Python 🐍 + Streamlit 🚀 + NLTK 🧠

#### ✅ Features:
- Accepts paragraph input
- Displays:
  - Word Tokens
  - Sentence Tokens
  - Total word count
- Simple UI using Streamlit

#### 📂 Files:
- `tokenizer_app.py`

#### ▶️ How to Run:
```bash
streamlit run tokenizer_app.py
